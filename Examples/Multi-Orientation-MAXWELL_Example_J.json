{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83eb39d9-fcbb-42a6-b39f-d4c714e5de35",
   "metadata": {},
   "source": [
    "# Example Application\n",
    "<b> Authored by Sierra Dean<br>\n",
    "06 December 2024<br>\n",
    "email: ccnd@live.com</b>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This document serves as a general walkthrough of how to use the Multi-Orientation MAXWELL analysis method. In addition to custom software hosted as various python classes, simulated data is provided to ensure a thorough understanding of the software capabilities.<br>\n",
    "Software:<br>\n",
    "<i>https://github.com/SierraD/Multi-Orientation-Maxwell/</i><br>\n",
    "Simulated Data: <br>\n",
    "<i>https://figshare.com/authors/Sierra_Dean/20319102</i><br>\n",
    "For this example, all classes composing the Multi-Orientation MAXWELL software are copied directly from the online repository into Jupyter Notebook cells. The classes can also be ran through the command line interface. Both the code, as well as the output, are recorded here as a general recommendation on how to execute the analysis method.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "The data used in this example is scintillation events simulated from ThunderSTORM. The example data uses a pixel size of 230 nm, which simulates a 20x magnification lens, and a 25 nm step between images, depicting the in-depth pixel size. The scintillation events have a FWHM between 2-5 pixels in size, with intensity between 10000-20000 photons per event.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee5f6e-f183-4a16-a2bb-e2c61a913ce2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b4464-f870-4aae-97e2-52cdab55fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in Preparation;\n",
    "import pandas\n",
    "# Used in Overlap;\n",
    "import numpy\n",
    "# Used in Plotting;\n",
    "import plotly\n",
    "from plotly import subplots\n",
    "# Used in Precision;\n",
    "import fitter\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e754da-2809-4d2a-95cd-32ea03881040",
   "metadata": {},
   "source": [
    "# Preparation;\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This class is used to format the two different orientation ThunderSTORM result files into readable data that can be called by the other classes in the software.<br>\n",
    "The input for this class is two different orientation ThunderSTORM results files, namely XY and XZ. When performing ThunderSTORM analysis, the pixel size parameter is set to 1 to accommodate the asymmetric voxel size between the two different orientations. This class is used to correct the isotropic voxel size to the correct nanometer units, which requires input of the in-plane pixel size, obtained from the camera and magnification, as well as the in-depth pixel size, which is the step distance between successive images. These are titled as pixelsize_xy and pixelsize_xz, respectively. <br>\n",
    "his class also enables re-centering the data around zero, and limiting the data to a dedicated ROI to reduce the computation time. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bc684-8c3c-4d69-866c-7e24593f61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preparation(object):\n",
    "    \"\"\"\n",
    "    This file is part of the Multi-Orientation MAXWELL software\n",
    "    \n",
    "    File author(s): Sierra Dean <ccnd@live.com>\n",
    "    \n",
    "    Distributed under the GPLv3 Licence.\n",
    "    See accompanying file LICENSE.txt or copy at\n",
    "        http://www.gnu.org/licenses/gpl-3.0.html\n",
    "        \n",
    "    source: https://github.com/SierraD/Multi-Orientation-Maxwell\n",
    "    \n",
    "    Last Updated: Sept 26 2024\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        A technique to prepare multi two-dimensional ThunderSTORM analysis files for three-dimensional\n",
    "        analysis. \n",
    "        \n",
    "        This method requires two different orientation files obtained from ThunderSTORM, containing\n",
    "        two-dimensional pixel-value localizations, and requires the user to input the voxel size\n",
    "        of the images by defining the in-plane pixel size, as well as the in-depth step size between \n",
    "        subsequent images. \n",
    "        \n",
    "        The values in the ThunderSTORM files will then be corrected from pixel size to the nanometer\n",
    "        scale, and can be set to a zero position, or limited to a user-defined region of interest, \n",
    "        as well as downloaded as a CSV file.\n",
    "        \n",
    "        Attributes:\n",
    "            None.\n",
    "            \n",
    "        Return: \n",
    "            None.\n",
    "        \"\"\"\n",
    "        return \n",
    "    \n",
    "    def setting(self, file_xy, file_xz, magnification=20, pixelsize_xy=230, pixelsize_xz=13, TS_dims = 2):\n",
    "        \"\"\"\n",
    "        A technique to download two different orientation two-dimensional ThunderSTORM analysis files\n",
    "        and correct the scale from pixel size to nanometers.\n",
    "                \n",
    "        Attributes:\n",
    "        file_xy & file_xz: str \"XY_File.csv\", \"XZ_File.csv\", etc.\n",
    "            The name of the ThunderSTORM results table obtained from the XY and XZ orientations. \n",
    "        magnification: int\n",
    "            The magnification of the lens used to obtain the images. If not specified, 20x magnification \n",
    "            will be assumed. \n",
    "        pixelsize_xy: int \n",
    "            The pixel size of the XY data in nm, calculated using the camera's pixel size, divided by \n",
    "            the magnification.\n",
    "            Example) Hamamatsu Quest at 20x magnificaiton = 4600 [nm] / 20 = 230 [nm] \n",
    "                    If not specified, this will be the value assumed.\n",
    "        pixelsize_xz: int\n",
    "            The Z step size in nm between images obtained from the LSM.\n",
    "            If not specified, 13 nm will be assumed.\n",
    "        TS_dims: 2 or 3\n",
    "            The number of positional dimensions specified in ThunderSTORM using the Z-stage Offset Menu. \n",
    "            If the third dimension was previously specified with correct Z step, no voxel adjustments will \n",
    "            be made.\n",
    "            \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        self.name_xy = file_xy\n",
    "        self.name_xz = file_xz\n",
    "        self.xypix = pixelsize_xy\n",
    "        self.zpix = pixelsize_xz\n",
    "        self.magnification = magnification\n",
    "        self.df_xy = pandas.read_csv(self.name_xy)\n",
    "        self.df_xz = pandas.read_csv(self.name_xz)\n",
    "        self.dfxy = pandas.concat([self.df_xy[\"x [nm]\"]*self.xypix,\n",
    "                               self.df_xy[\"y [nm]\"]*self.xypix,\n",
    "                               self.df_xy[\"uncertainty [nm]\"]*self.xypix,\n",
    "                               self.df_xy[\"intensity [photon]\"],\n",
    "                               self.df_xy[\"offset [photon]\"],\n",
    "                               self.df_xy[\"bkgstd [photon]\"],\n",
    "                               self.df_xy[\"sigma [nm]\"]*self.xypix], \n",
    "                               keys=[\"X_XY\", \"Y_XY\", \"U_XY\", \"I_XY\", \"O_XY\", \"B_XY\", \"S_XY\"], axis=1)\n",
    "        self.dfxz = pandas.concat([self.df_xz[\"x [nm]\"]*self.xypix, \n",
    "                               self.df_xz[\"y [nm]\"]*self.zpix,\n",
    "                               self.df_xz[\"uncertainty [nm]\"]*self.xypix, \n",
    "                               self.df_xz[\"uncertainty [nm]\"]*self.zpix, \n",
    "                               self.df_xz[\"intensity [photon]\"],\n",
    "                               self.df_xz[\"offset [photon]\"],\n",
    "                               self.df_xz[\"bkgstd [photon]\"],\n",
    "                               self.df_xz[\"sigma [nm]\"]*self.xypix, \n",
    "                               self.df_xz[\"sigma [nm]\"]*self.zpix], \n",
    "                               keys=[\"X_XZ\", \"Z_XZ\", \"U_X\", \"U_Z\",\"I_XZ\", \"O_XZ\", \"B_XZ\", \"S_X\", \"S_Z\"], axis=1)\n",
    "        if TS_dims == 2:\n",
    "            self.dfxy.insert(2, \"Z_XY\", (self.df_xy[\"frame\"]*self.zpix))\n",
    "            self.dfxz.insert(1, \"Y_XZ\", (self.df_xz[\"frame\"]*self.xypix))\n",
    "        elif TS_dims == 3:\n",
    "            self.dfxy.insert(2, \"Z_XY\", self.df_xy[\"frame\"])\n",
    "            self.dfxz.insert(1, \"Y_XZ\", self.df_xz[\"frame\"])\n",
    "        self.xy_len = len(self.dfxy)\n",
    "        self.xz_len = len(self.dfxz)\n",
    "        return self\n",
    "    \n",
    "    def set_to_center(self):\n",
    "        \"\"\"\n",
    "        A technique to center the data to a zero center position.\n",
    "        \n",
    "        Attributes:\n",
    "            None.\n",
    "        \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        self.dfxz[\"X_XZ\"] = self.dfxz[\"X_XZ\"]-(max(self.dfxy[\"X_XY\"])+min(self.dfxy[\"X_XY\"]))/2\n",
    "        self.dfxy[\"X_XY\"] = self.dfxy[\"X_XY\"]-(max(self.dfxy[\"X_XY\"])+min(self.dfxy[\"X_XY\"]))/2\n",
    "        self.dfxz[\"Y_XZ\"] = self.dfxz[\"Y_XZ\"]-(max(self.dfxy[\"Y_XY\"])+min(self.dfxy[\"Y_XY\"]))/2\n",
    "        self.dfxy[\"Y_XY\"] = self.dfxy[\"Y_XY\"]-(max(self.dfxy[\"Y_XY\"])+min(self.dfxy[\"Y_XY\"]))/2\n",
    "        self.dfxy[\"Z_XY\"] = self.dfxy[\"Z_XY\"]-max(self.dfxz[\"Z_XZ\"])/2\n",
    "        self.dfxz[\"Z_XZ\"] = self.dfxz[\"Z_XZ\"]-max(self.dfxz[\"Z_XZ\"])/2\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def limiting(self, axis, limit, direction):\n",
    "        \"\"\"\n",
    "        A technique to limit the data to a specified positional region of interest.\n",
    "        \n",
    "        Attributes:\n",
    "        axis: str \"X\", \"Y\", \"Z\"\n",
    "            The axis along which the data will be limited.\n",
    "        limit: num\n",
    "            The number value which is the limiting factor for the data.\n",
    "        direction: str \"Less\", \"Lesser\", \"More\", \"Greater\", etc.\n",
    "            The direction in which the data will be limited. If \"lesser\" is specified, all values above the \n",
    "            limit will be removed, and vice versa if \"greater\" is specified.\n",
    "    \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        if direction == (\"less\" or \"Less\" or \"lesser\" or \"Lesser\"):\n",
    "            indexes_xy = [idy for idy, valuey in enumerate(self.dfxy[axis+\"_XY\"]) if valuey >= limit]\n",
    "            indexes_xz = [idz for idz, valuez in enumerate(self.dfxz[axis+\"_XZ\"]) if valuez >= limit]\n",
    "            self.dfxy = self.dfxy.drop(indexes_xy).reset_index(drop=True)\n",
    "            self.dfxz = self.dfxz.drop(indexes_xz).reset_index(drop=True)\n",
    "        elif direction == (\"more\" or \"More\" or \"greater\" or \"Greater\"):\n",
    "            indexes_xy = [idy for idy, valuey in enumerate(self.dfxy[axis+\"_XY\"]) if valuey <= limit]\n",
    "            indexes_xz = [idz for idz, valuez in enumerate(self.dfxz[axis+\"_XZ\"]) if valuez <= limit]\n",
    "            self.dfxy = self.dfxy.drop(indexes_xy).reset_index(drop=True)\n",
    "            self.dfxz = self.dfxz.drop(indexes_xz).reset_index(drop=True)\n",
    "        return self\n",
    "    \n",
    "    def download_dataframe(self, filename=\"Preparation_Dataframe\"):\n",
    "        \"\"\"\n",
    "        A technique to download the data prepared by the preparation.py method as a CSV file.\n",
    "        \n",
    "        Attributes:\n",
    "            None.\n",
    "            \n",
    "        Return:\n",
    "            None. Will download the dataframe as a CSV file.\n",
    "        \"\"\"\n",
    "        download_df = pandas.concat([self.dfxy,self.dfxz],axis=1,sort=False)\n",
    "        download_df = download_df.rename(columns={'X_XY': 'x_xy [nm]', \n",
    "                                                  'Y_XY': 'y_xy [nm]', \n",
    "                                                  'Z_XY': 'z_xy [nm]', \n",
    "                                                  'U_XY': 'uncertainty_xy [nm]', \n",
    "                                                  'I_XY': 'intensity_xy [photon]',\n",
    "                                                  'O_XY': 'offset_xy [photon]',\n",
    "                                                  'B_XY': 'bkgstd_xy [photon]',\n",
    "                                                  'S_XY': 'sigma_xy [nm]', \n",
    "                                                  'X_XZ': 'x_xz [nm]', \n",
    "                                                  'Y_XZ': 'y_xz [nm]', \n",
    "                                                  'Z_XZ': 'z_xz [nm]', \n",
    "                                                  'U_X': 'uncertainty_x [nm]',\n",
    "                                                  'U_Z': 'uncertainty_z [nm]',\n",
    "                                                  'I_XZ': 'intensity_xz [photon]',\n",
    "                                                  'O_XZ': 'offset_xz [photon]',\n",
    "                                                  'B_XZ': 'bkgstd_xz [photon]',\n",
    "                                                  'S_X': 'sigma_x [nm]', \n",
    "                                                  'S_Z': 'sigma_z [nm]'})\n",
    "        download_df.to_csv(filename+\".csv\", index=False, encoding='utf-8')\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b17e9-96d5-4026-af1e-121dd5afef50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The following is the code to initate the Preparation.py class, using two ThunderSTORM results files obtained by analyzing two different orientations of the dame data. This class requires the camera pixel size, which is the XY orientation, in addition to the step size between images, which is designated as the XZ pixel size.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aee4e3-3d7b-4c84-b990-e1abc49b0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prepare = preparation() \n",
    "Prepare = Prepare.setting(\"ThunderSTORM_Results_XY.csv\", \n",
    "                          \"ThunderSTORM_Results_XZ.csv\", \n",
    "                          magnification=20, # Camera magnificaion used during experimentation\n",
    "                          pixelsize_xy=230, # Camera pixel size during image acquisition\n",
    "                          pixelsize_xz=60)  # Step size between successive images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff6b75",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The data inside of the class is not visible until an internal parameter is called. An example is shown below, which will print all of the X positions obtained from the XY orientation ThunderSTORM results file. This code can be altered to print a variety of internal parameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Prepare.dfxy[\"X_XY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5f993-eafb-4b22-83a1-2bc6186194ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This class also allows for the data to be re-centered around zero in all directions by using the set_to_center command.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb49d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before centering\n",
    "print(Prepare.dfxy[\"X_XY\"].min())\n",
    "print(Prepare.dfxy[\"X_XY\"].max())\n",
    "\n",
    "# After centering\n",
    "Centered = Prepare.set_to_center()\n",
    "print(Centered.dfxy[\"X_XY\"].min())\n",
    "print(Centered.dfxy[\"X_XY\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be793a35-c45a-445e-b88c-2c3dfa81ba23",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In addition, if the ThunderSTORM results files contain a large number of localizations, the data can be reduced to a region of interest to reduce the computation time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Centered.limiting(\"X\", 15000, \"less\")\n",
    "Data = Data.limiting(\"X\", -15000, \"more\")\n",
    "Data = Data.limiting(\"Y\", 15000, \"less\")\n",
    "Data = Data.limiting(\"Y\", -15000, \"more\")\n",
    "\n",
    "print(Data.dfxy[\"X_XY\"].min())\n",
    "print(Data.dfxy[\"X_XY\"].max())\n",
    "print(Data.dfxy[\"Y_XY\"].min())\n",
    "print(Data.dfxy[\"Y_XY\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd0321",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The data, formatted by this class, can be downloaded as a csv file using the download_dataframe command.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c6183-2ce3-48cc-8abf-21b7d8f1565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.download_dataframe(filename=\"Preparation_Dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b750333",
   "metadata": {},
   "source": [
    "# Overlap;\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This class is used to determine the position matching events between the two different orientations. This requires the data to have been formatted in the correct nanometer units with the Preparation.py class. To find the position matched events, the indexes of the overlapped events are first determined, then the values associated with those indexes are saved for future filtering. <br>\n",
    "To determine the overlapped, or position matched events between the two orientations, the position in all three dimensions is evaluated. For both the XY and XZ localization analysis, each event has a three dimensional position, as well as associated positional uncertainty. To find the position matched events, each localization in the first orientation is compared with every localization from the second orientation. If a pair of localizations from both XY and XZ orientations overlap in all three dimensions, including overlap from their positional uncertainties, the pair is saved as a position matched event.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05120191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class overlap(object):\n",
    "    \"\"\"\n",
    "    This file is part of the Multi-Orientation MAXWELL software\n",
    "    \n",
    "    File author(s): Sierra Dean <ccnd@live.com>\n",
    "    \n",
    "    Distributed under the GPLv3 Licence.\n",
    "    See accompanying file LICENSE.txt or copy at\n",
    "        http://www.gnu.org/licenses/gpl-3.0.html\n",
    "        \n",
    "    source: https://github.com/SierraD/Multi-Orientation-MAXWELL\n",
    "    \n",
    "    Last Updated: Sept 26 2024\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        A technique to use two different orientations of ThunderSTORM results to determine the overlapping \n",
    "        points in 3D space.\n",
    "        \n",
    "        This method splits the calculation into determining the indexes of the overlap, then determining\n",
    "        the localizations using the indexes, which is computationally easier than performing both \n",
    "        within the same method.\n",
    "        \n",
    "        Attributes:\n",
    "        data: \n",
    "            The data previously developed and contained within the Preparations.py class.\n",
    "            \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.zpix = self.data.zpix\n",
    "        self.xypix = self.data.xypix\n",
    "        self.magnification = self.data.magnification\n",
    "        return \n",
    "    \n",
    "    def indexes(self, z_range, xy_range):\n",
    "        \"\"\"\n",
    "        A technique to determine the indexes of the dataframe where the points from the two different \n",
    "        orientations overlap in 3D space. \n",
    "        \n",
    "        For each localization, a 3D sphere is defined around each position, with the radius in each direction\n",
    "        determined by that point's uncertainty value in that direction. For uncertain values (i.e. Z in XY and \n",
    "        Y in XZ), the uncertainty is taken as the pixel size, or a user-specified value. \n",
    "        \n",
    "        After a 3D positional uncertainty sphere has been determined for each point in either dataframe, \n",
    "        the two orientations are compared, and points which overlap in 3D space from both orientations\n",
    "        are saved as overlapped indexes.\n",
    "        \n",
    "        Attributes:\n",
    "        z_range: int\n",
    "            The radius of the Z undertainty for the 3D sphere for the XY data, which does not naturally\n",
    "            include Z uncertainty values. \n",
    "            The recommended values are either the Z step [nm] used between successive images, or the \n",
    "            width of the light sheet, which designates the in-depth resolution of the system.\n",
    "        xy_range: int\n",
    "            The radius of the Y uncertainty for the 3D sphere for the XZ data, which does not naturally\n",
    "            include Y uncertainty values.\n",
    "            The recommended value is the in-plane pixel size [nm], which designates the Y step [nm]\n",
    "            when the in-plane data is subjected to a pixelwise transformation without interpolation.\n",
    "            \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        range_x_xz = []\n",
    "        range_x_xy = []\n",
    "        range_y_xz = []\n",
    "        range_y_xy = []\n",
    "        range_z_xz = []\n",
    "        range_z_xy = []\n",
    "        for i in range(0, len(self.data.dfxz[\"X_XZ\"])):\n",
    "            x_left_xz = (self.data.dfxz[\"X_XZ\"][i]-self.data.dfxz[\"U_X\"][i])\n",
    "            x_right_xz = (self.data.dfxz[\"X_XZ\"][i]+self.data.dfxz[\"U_X\"][i])\n",
    "            y_left_xz = (self.data.dfxz[\"Y_XZ\"][i]-xy_range)\n",
    "            y_right_xz = (self.data.dfxz[\"Y_XZ\"][i]+xy_range)\n",
    "            z_left_xz = (self.data.dfxz[\"Z_XZ\"][i]-self.data.dfxz[\"U_Z\"][i])\n",
    "            z_right_xz = (self.data.dfxz[\"Z_XZ\"][i]+self.data.dfxz[\"U_Z\"][i])\n",
    "            range_x_xz.append((x_left_xz, x_right_xz))\n",
    "            range_y_xz.append((y_left_xz, y_right_xz))\n",
    "            range_z_xz.append((z_left_xz, z_right_xz))\n",
    "        RX_xz = pandas.arrays.IntervalArray.from_tuples(range_x_xz)\n",
    "        RY_xz = pandas.arrays.IntervalArray.from_tuples(range_y_xz)\n",
    "        RZ_xz = pandas.arrays.IntervalArray.from_tuples(range_z_xz)\n",
    "        new_columns_data = {\"X Range XZ\": RX_xz, \n",
    "                            \"Y Range XZ\": RY_xz, \n",
    "                            \"Z Range XZ\": RZ_xz}  \n",
    "        new_columns_df = pandas.DataFrame(new_columns_data)\n",
    "        self.data.dfxz = pandas.concat([self.data.dfxz, new_columns_df], axis=1)\n",
    "        for j in range(0, len(self.data.dfxy[\"X_XY\"])):\n",
    "            x_left_xy = (self.data.dfxy[\"X_XY\"][j]-self.data.dfxy[\"U_XY\"][j])\n",
    "            x_right_xy = (self.data.dfxy[\"X_XY\"][j]+self.data.dfxy[\"U_XY\"][j])\n",
    "            y_left_xy = (self.data.dfxy[\"Y_XY\"][j]-self.data.dfxy[\"U_XY\"][j])\n",
    "            y_right_xy = (self.data.dfxy[\"Y_XY\"][j]+self.data.dfxy[\"U_XY\"][j])\n",
    "            z_left_xy = (self.data.dfxy[\"Z_XY\"][j]-z_range)\n",
    "            z_right_xy = (self.data.dfxy[\"Z_XY\"][j]+z_range)\n",
    "            range_x_xy.append((x_left_xy, x_right_xy))\n",
    "            range_y_xy.append((y_left_xy, y_right_xy))\n",
    "            range_z_xy.append((z_left_xy, z_right_xy))\n",
    "        RX_xy = pandas.arrays.IntervalArray.from_tuples(range_x_xy)\n",
    "        RY_xy = pandas.arrays.IntervalArray.from_tuples(range_y_xy)\n",
    "        RZ_xy = pandas.arrays.IntervalArray.from_tuples(range_z_xy)\n",
    "        new_columns_datay = {\"X Range XY\": RX_xy, \n",
    "                             \"Y Range XY\": RY_xy, \n",
    "                             \"Z Range XY\": RZ_xy}  \n",
    "        new_columns_dfy = pandas.DataFrame(new_columns_datay)\n",
    "        self.data.dfxy = pandas.concat([self.data.dfxy, new_columns_dfy], axis=1)\n",
    "        all_indexes_XY = []\n",
    "        all_indexes_XZ = []\n",
    "        for k in range(0, len(self.data.dfxz[\"X_XZ\"])):\n",
    "            X_overlap = numpy.where(RX_xy.overlaps(RX_xz[k]))\n",
    "            if X_overlap[0].size != 0:\n",
    "                Y_overlap = numpy.where(RY_xy.overlaps(RY_xz[k]))\n",
    "                xy = numpy.intersect1d(X_overlap, Y_overlap)\n",
    "                if xy.size != 0:\n",
    "                    Z_overlap = numpy.where(RZ_xy.overlaps(RZ_xz[k]))\n",
    "                    xyz = numpy.intersect1d(xy, Z_overlap)\n",
    "                    if xyz.size != 0:\n",
    "                        value = [k]*len(xyz)\n",
    "                        all_indexes_XY.append(xyz.tolist())\n",
    "                        all_indexes_XZ.append(value)\n",
    "        self.XY_indexes = sum(all_indexes_XY, []) \n",
    "        self.XZ_indexes = sum(all_indexes_XZ, [])\n",
    "        return self\n",
    "    \n",
    "    def values(self):\n",
    "        \"\"\"\n",
    "        A technique to determine the positional information using the indexes of overlap\n",
    "        determined within the method.\n",
    "        \n",
    "        Attributes:\n",
    "            None.\n",
    "        Return:\n",
    "            None. Will modify the data established in place. \n",
    "        \"\"\"\n",
    "        df = pandas.DataFrame(columns=[\"X_XY\", \"Y_XY\", \"Z_XY\", \n",
    "                                   \"U_XY\", \"I_XY\", \"O_XY\", \"B_XY\", \"S_XY\", \n",
    "                                   \"X_XZ\", \"Y_XZ\", \"Z_XZ\", \n",
    "                                   \"U_X\", \"U_Z\", \"I_XZ\", \"O_XZ\", \"B_XZ\", \"S_X\", \"S_Z\"])\n",
    "        for i in range(0, len(self.XY_indexes)):\n",
    "            X_XY = self.data.dfxy[\"X_XY\"][self.XY_indexes[i]]\n",
    "            Y_XY = self.data.dfxy[\"Y_XY\"][self.XY_indexes[i]]\n",
    "            Z_XY = self.data.dfxy[\"Z_XY\"][self.XY_indexes[i]]\n",
    "            U_XY = self.data.dfxy[\"U_XY\"][self.XY_indexes[i]]\n",
    "            I_XY = self.data.dfxy[\"I_XY\"][self.XY_indexes[i]]\n",
    "            O_XY = self.data.dfxy[\"O_XY\"][self.XY_indexes[i]]\n",
    "            B_XY = self.data.dfxy[\"B_XY\"][self.XY_indexes[i]]\n",
    "            S_XY = self.data.dfxy[\"S_XY\"][self.XY_indexes[i]]\n",
    "            X_XZ = self.data.dfxz[\"X_XZ\"][self.XZ_indexes[i]]\n",
    "            Y_XZ = self.data.dfxz[\"Y_XZ\"][self.XZ_indexes[i]]\n",
    "            Z_XZ = self.data.dfxz[\"Z_XZ\"][self.XZ_indexes[i]]\n",
    "            U_X = self.data.dfxz[\"U_X\"][self.XZ_indexes[i]]\n",
    "            U_Z = self.data.dfxz[\"U_Z\"][self.XZ_indexes[i]]\n",
    "            I_XZ = self.data.dfxz[\"I_XZ\"][self.XZ_indexes[i]]\n",
    "            O_XZ = self.data.dfxz[\"O_XZ\"][self.XZ_indexes[i]]\n",
    "            B_XZ = self.data.dfxz[\"B_XZ\"][self.XZ_indexes[i]]\n",
    "            S_X = self.data.dfxz[\"S_X\"][self.XZ_indexes[i]]\n",
    "            S_Z = self.data.dfxz[\"S_Z\"][self.XZ_indexes[i]]\n",
    "            df.loc[i] = ([X_XY]+[Y_XY]+[Z_XY]+[U_XY]+[I_XY]+[O_XY]+[B_XY]+[S_XY]+\n",
    "                         [X_XZ]+[Y_XZ]+[Z_XZ]+[U_X]+[U_Z]+[I_XZ]+[O_XZ]+[B_XZ]+[S_X]+[S_Z])\n",
    "        self.df=df\n",
    "        self.dfxy = pandas.concat([df[\"X_XY\"], df[\"Y_XY\"], df[\"Z_XY\"], \n",
    "                                   df[\"U_XY\"], df[\"I_XY\"], df[\"O_XY\"], df[\"B_XY\"], df[\"S_XY\"]], \n",
    "                            keys=[\"X_XY\", \"Y_XY\", \"Z_XY\", \"U_XY\", \"I_XY\", \"O_XY\", \"B_XY\", \"S_XY\"], axis=1)\n",
    "        self.dfxz = pandas.concat([df[\"X_XZ\"], df[\"Y_XZ\"], df[\"Z_XZ\"], \n",
    "                                   df[\"U_X\"], df[\"U_Z\"], df[\"I_XY\"], df[\"O_XY\"], df[\"B_XY\"], df[\"S_X\"], df[\"S_Z\"]], \n",
    "                            keys=[\"X_XZ\", \"Y_XZ\", \"Z_XZ\", \"U_X\", \"U_Z\", \"I_XZ\", \"O_XZ\", \"B_XZ\", \"S_X\", \"S_Z\"], axis=1)\n",
    "        return self\n",
    "\n",
    "    def download_dataframe(self, filename=\"Overlap_Dataframe\"):\n",
    "        \"\"\"\n",
    "        A technique to download the data prepared by the Overlap.py method as a CSV file named\n",
    "        \"Overlap_Dataframe.csv\".\n",
    "        \n",
    "        Attributes:\n",
    "            None.\n",
    "        Return:\n",
    "            None. Will download the dataframe as a CSV file.\n",
    "        \"\"\"\n",
    "        download_df = pandas.concat([self.dfxy, self.dfxz], axis=1, sort=False)\n",
    "        download_df = download_df.rename(columns={'X_XY': 'x_xy [nm]', \n",
    "                                                  'Y_XY': 'y_xy [nm]', \n",
    "                                                  'Z_XY': 'z_xy [nm]', \n",
    "                                                  'U_XY': 'uncertainty_xy [nm]', \n",
    "                                                  'I_XY': 'intensity_xy [photon]',\n",
    "                                                  'O_XY': 'offset_xy [photon]',\n",
    "                                                  'B_XY': 'bkgstd_xy [photon]',\n",
    "                                                  'S_XY': 'sigma_xy [nm]', \n",
    "                                                  'X_XZ': 'x_xz [nm]', \n",
    "                                                  'Y_XZ': 'y_xz [nm]', \n",
    "                                                  'Z_XZ': 'z_xz [nm]', \n",
    "                                                  'U_X': 'uncertainty_x [nm]',\n",
    "                                                  'U_Z': 'uncertainty_z [nm]',\n",
    "                                                  'I_XZ': 'intensity_xz [photon]',\n",
    "                                                  'O_XZ': 'offset_xz [photon]',\n",
    "                                                  'B_XZ': 'bkgstd_xz [photon]',\n",
    "                                                  'S_X': 'sigma_x [nm]', \n",
    "                                                  'S_Z': 'sigma_z [nm]'})\n",
    "        download_df.to_csv(filename+\".csv\", index=False, encoding='utf-8')\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da3e7b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To find the position matched events, the three-dimensional position of each localization in the first orientation is compared with every localization from the second orientation. A pair of localizations between XY and XZ is only considered a position matched pair if their X, Y, and Z positions all overlap, within positional uncertainty values.<br>\n",
    "In the XY orientation, the X and Y positional uncertainty values are calculated by ThunderSTORM, and are deemed accurate. On the other hand, the Z positional uncertainty is determined by the step size, and is much larger. Similarly with XZ, the Y positional uncertainty is the pixel size, and is very large. To increase or decrease the large positional uncertainty values used to determine the overlap between XY and XZ localizations, the z_range and xy_range can be provided when initiating the Overlap.py class. Generally, the experimental parameters are provided, as shown below. The indexes of the overlapped events are first determined with the indexes command, with the localization information then assigned with the values command.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea544a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matched = overlap(Data)\n",
    "Indexes = Matched.indexes(z_range = Data.zpix , xy_range = Data.xypix)\n",
    "Values = Indexes.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6b511",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The indexes command creates a dataframe of all of the indexes for the position matched pairs. In this example, the first position matched pair of localizations is the 61st localization from the XY results file, and the 10th localization from the XZ results file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Indexes.XY_indexes[0])\n",
    "print(Indexes.XZ_indexes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd688eaf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The values command reads the indexes, and acquires the data from the original ThunderSTORM results files for those designated indexes, returning them as a dataframe. In the following code, it can be seen that the dataframe from the values command gives the same information as the Preparation.py dataframe when using the indexes from the indexes command.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Values.df[\"X_XY\"][0])\n",
    "print(Values.df[\"U_XY\"][0])\n",
    "print(Values.df[\"X_XZ\"][0])\n",
    "print(Values.df[\"U_X\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Data.dfxy[\"X_XY\"][61])\n",
    "print(Data.dfxy[\"U_XY\"][61])\n",
    "print(Data.dfxz[\"X_XZ\"][10])\n",
    "print(Data.dfxz[\"U_X\"][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da874d8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The above code demonstrates only the X positions (XY and XZ) and their positional uncertainties for the first position-matched pair. To be considered a position-matched pair, the data must overlap in all three dimensions. The following figure depicts the X Positions (Top), Y Positions (Middle), and Z Positions (Bottom), shown as black circles. The positional uncertainty obtained from the XY (Red) and XZ (Blue) orientations are also shown as error bars originating from the positions. This is only demonstrated for the first position matched pair, but this type of three-dimensional overlap is required for all pairs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0babbf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only used for this demonstration, not necessary for the software\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ([ax1, ax2, ax3]) = plt.subplots(3, 1, figsize=(10,5))\n",
    "fig.tight_layout(pad=3)\n",
    "\n",
    "ax1.set_ylim(-3, 5)\n",
    "ax1.yaxis.set_ticklabels([])\n",
    "ax1.hlines(-1, (Values.df[\"X_XY\"][0] - Values.df[\"U_XY\"][0]), (Values.df[\"X_XY\"][0] + Values.df[\"U_XY\"][0]), \n",
    "           colors=\"red\", label=\"XY\")\n",
    "ax1.hlines(1, (Values.df[\"X_XZ\"][0] - Values.df[\"U_X\"][0]), (Values.df[\"X_XZ\"][0] + Values.df[\"U_X\"][0]), \n",
    "           colors=\"Blue\", label=\"XZ\")\n",
    "ax1.legend(bbox_to_anchor=(1.12, 1.))\n",
    "ax1.scatter(Values.df[\"X_XY\"][0], -1, color=\"Black\")\n",
    "ax1.scatter(Values.df[\"X_XZ\"][0], 1, color=\"Black\")\n",
    "ax1.set_xlabel(u\"X Position \\u00B1 X Positional Uncertainty [nm]\")\n",
    "\n",
    "ax2.set_ylim(-3, 5)\n",
    "ax2.yaxis.set_ticklabels([])\n",
    "ax2.hlines(-1, (Values.df[\"Y_XY\"][0] - Values.df[\"U_XY\"][0]), (Values.df[\"Y_XY\"][0] + Values.df[\"U_XY\"][0]), \n",
    "           colors=\"red\", label=\"XY\")\n",
    "ax2.hlines(1, (Values.df[\"Y_XZ\"][0] - Values.xypix), (Values.df[\"Y_XZ\"][0] + Values.xypix), \n",
    "           colors=\"Blue\", label=\"XZ\")\n",
    "ax2.scatter(Values.df[\"Y_XY\"][0], -1, color=\"Black\")\n",
    "ax2.scatter(Values.df[\"Y_XZ\"][0], 1, color=\"Black\")\n",
    "ax2.set_xlabel(u\"Y Position \\u00B1 Y Positional Uncertainty [nm]\")\n",
    "\n",
    "ax3.set_ylim(-3, 5)\n",
    "ax3.yaxis.set_ticklabels([])\n",
    "ax3.hlines(-1, (Values.df[\"Z_XY\"][0] - Values.zpix), (Values.df[\"Z_XY\"][0] + Values.zpix), \n",
    "           colors=\"red\", label=\"XY\")\n",
    "ax3.hlines(1, (Values.df[\"Z_XZ\"][0] - Values.df[\"U_Z\"][0]), (Values.df[\"Z_XZ\"][0] + Values.df[\"U_Z\"][0]), \n",
    "           colors=\"Blue\", label=\"XZ\")\n",
    "ax3.scatter(Values.df[\"Z_XY\"][0], -1, color=\"Black\")\n",
    "ax3.scatter(Values.df[\"Z_XZ\"][0], 1, color=\"Black\")\n",
    "ax3.set_xlabel(u\"Z Position \\u00B1 Z Positional Uncertainty [nm]\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c106f67",
   "metadata": {},
   "source": [
    "# Filtering;\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This class is used to filter the position matched events to ensure no duplicate events exist. Duplicate events are seen when one localization from the XY dataframe is position matched with various localizations from the XZ dataframe, or vise versa. Duplicate events can occur most frequently when scanned with the X-ray light sheet tail. <br>\n",
    "It has been observed that when duplicate events occur, the multiple localizations from a single set have drastically increasing or decreasing positional uncertainty, allowing for the selection of the lowest positional uncertainty as the particle's true location out of all of the duplicates.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class filtering(object):\n",
    "    \"\"\"\n",
    "    This file is part of the Multi-Orientation MAXWELL software\n",
    "    \n",
    "    File author(s): Sierra Dean <ccnd@live.com>\n",
    "    \n",
    "    Distributed under the GPLv3 Licence.\n",
    "    See accompanying file LICENSE.txt or copy at\n",
    "        http://www.gnu.org/licenses/gpl-3.0.html\n",
    "        \n",
    "    source: https://github.com/SierraD/Multi-Orientation-MAXWELL\n",
    "    \n",
    "    Last Updated: Sept 26 2024\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        A technique to obtain precise localizations in 3D space, with data originating \n",
    "        from two-dimensional ThunderSTORM analysis performed on mulitple orientations. \n",
    "        \n",
    "        This method uses the overlapped localizations determined by the Overlap.py method and \n",
    "        filters all overlapped points to ensure no duplications. The choice of filtering\n",
    "        includes selecting the lowest uncertainty values, or the most similar intensity\n",
    "        values. \n",
    "        \n",
    "        The final dataframe obtained from this method is the resulting 3D localizations\n",
    "        obtained from comparing the two ThunderSTORM orientations.\n",
    "        \n",
    "        Attributes:\n",
    "        data: \n",
    "            The data previously developed and contained within the Overlap.py class.\n",
    " \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        return \n",
    "    \n",
    "    def merge(self):\n",
    "        \"\"\"\n",
    "        A technique to group indexes which contain points which overlap with multiple other\n",
    "        points, which can then be used to filter out duplicates.\n",
    "        \n",
    "        Attributes:\n",
    "            None. \n",
    " \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        unique_XY = numpy.unique(self.data.df[\"X_XY\"])\n",
    "        unique_XZ = numpy.unique(self.data.df[\"X_XZ\"])\n",
    "        XY_indexes = []\n",
    "        XZ_indexes = []\n",
    "        for i in unique_XZ:\n",
    "            XZ_indexes.append(numpy.where(self.data.df[\"X_XZ\"]==i)[0].tolist())\n",
    "        for j in unique_XY:\n",
    "            XY_indexes.append(numpy.where(self.data.df[\"X_XY\"]==j)[0].tolist())\n",
    "        all_items = numpy.arange(0, len(self.data.df[\"X_XZ\"]))\n",
    "        all_indexes = []\n",
    "        for k in all_items:\n",
    "            list_all = numpy.array([k])\n",
    "            for l in list_all:\n",
    "                list_z = []\n",
    "                list_y = []\n",
    "                [list_z.append(sub_list) for sub_list in XZ_indexes if l in sub_list]\n",
    "                [list_y.append(sub_list) for sub_list in XY_indexes if l in sub_list]\n",
    "                list_all = numpy.unique(numpy.append(list_all, list_z))\n",
    "                list_all = numpy.unique(numpy.append(list_all, list_y))\n",
    "            all_indexes.append(list_all.tolist())\n",
    "        self.all_indexes = all_indexes\n",
    "        merged_indexes = []\n",
    "        for m in all_indexes:\n",
    "            m = set(m)\n",
    "            for n in merged_indexes:\n",
    "                if m & n:\n",
    "                    n.update(m)\n",
    "                    break\n",
    "            else:\n",
    "                merged_indexes.append(m)\n",
    "        merged_indexes = [list(o) for o in merged_indexes]\n",
    "        self.merged_indexes = merged_indexes\n",
    "        return self\n",
    "    \n",
    "    def selection(self, selection_type=\"uncertainty\"):\n",
    "        \"\"\"\n",
    "        A technique to filter all of the indexes which contain overlaps from multiple \n",
    "        localizations to remove all non-unique localizations.\n",
    "                \n",
    "        Attributes:\n",
    "        selection_type: str \"uncertainty\", \"Uncertainty\", \"intensity\", \"Intensity\"\n",
    "            The method of filtering. If uncertainty is selected, for all overlapped points, \n",
    "            only the lowest positional uncertainty point will be kept. If intensity is selected, \n",
    "            only the lowest intensity point will be kept.\n",
    " \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        if (selection_type==\"uncertainty\") or (selection_type==\"Uncertainty\"):\n",
    "            point_selection = []\n",
    "            for i in range(0, len(self.merged_indexes)):\n",
    "                u_xy = []\n",
    "                u_z = []\n",
    "                u_x = []\n",
    "                sumall = []\n",
    "                for j in self.merged_indexes[i]:\n",
    "                    u_xy.append(self.data.df[\"U_XY\"][j])\n",
    "                    u_z.append(self.data.df[\"U_Z\"][j])\n",
    "                    u_x.append(self.data.df[\"U_X\"][j])\n",
    "                [sumall.append(sum(s)) for s in zip(*[u_xy, u_z, u_x])]\n",
    "                loc = numpy.where(sumall==min(sumall))[0]\n",
    "                point_selection.append(self.merged_indexes[i][loc[0]])\n",
    "            self.point_indexes = point_selection\n",
    "        elif (selection_type==\"intensity\") or (selection_type==\"Intensity\"):\n",
    "            point_selection = []\n",
    "            for i in range(0, len(self.merged_indexes)):\n",
    "                i_xy = []\n",
    "                i_xz = []\n",
    "                sumall = []\n",
    "                for j in self.merged_indexes[i]:\n",
    "                    i_xy.append(self.data.df[\"I_XY\"][j])\n",
    "                    i_xz.append(self.data.df[\"I_XZ\"][j])\n",
    "                [sumall.append(sum(s)) for s in zip(*[i_xy, i_xz])]\n",
    "                loc = numpy.where(sumall==min(sumall))[0]\n",
    "                point_selection.append(self.merged_indexes[i][loc[0]])\n",
    "            self.point_indexes = point_selection\n",
    "        return self\n",
    "    \n",
    "    def points(self):\n",
    "        \"\"\"\n",
    "        A technique to obtain a dataframe of precise localizations in 3D space, \n",
    "        with data originating from two-dimensional ThunderSTORM analysis performed \n",
    "        in multiple orientations.\n",
    "        \n",
    "        Attributes:\n",
    "            None.\n",
    "            \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        df = self.data.df\n",
    "        for i in range(0, len(df)):\n",
    "            if i not in (self.point_indexes):\n",
    "                df = df.drop(i, axis=0)\n",
    "        self.df = df\n",
    "        points = pandas.concat([df[\"X_XY\"], df[\"Y_XY\"], df[\"Z_XZ\"], \n",
    "                            df[\"U_XY\"], df[\"U_Z\"], \n",
    "                            df[\"S_XY\"], df[\"S_Z\"], \n",
    "                            df[\"I_XY\"], df[\"I_XZ\"], \n",
    "                            df[\"O_XY\"], df[\"O_XZ\"],\n",
    "                            df[\"B_XY\"], df[\"B_XZ\"]], \n",
    "                           keys = [\"X [nm]\", \"Y [nm]\", \"Z [nm]\", \n",
    "                                   \"Uncertainty XY [nm]\", \"Uncertainty Z [nm]\",\n",
    "                                   \"Sigma XY [nm]\", \"Sigma Z [nm]\",\n",
    "                                   \"Intensity XY [Photons]\", \"Intensity XZ [Photons]\",\n",
    "                                   \"Offset XY [Photons]\", \"Offset XZ [Photons]\",\n",
    "                                   \"Bkgstd XY [Photons]\", \"Bkgstd XZ [Photons]\"], axis=1).reset_index(drop=True)\n",
    "        self.points = points\n",
    "        return self\n",
    "    \n",
    "    def download_dataframe(self, filename=\"Filtering_Dataframe\"):\n",
    "        \"\"\"\n",
    "        A technique to download the data prepared by the Filtering.py method as a \n",
    "        CSV file named \"Filtering_Dataframe.csv\".\n",
    "        \n",
    "        Attributes:\n",
    "            None.\n",
    "            \n",
    "        Return:\n",
    "            None. Will download the dataframe as a CSV file.\n",
    "        \"\"\"\n",
    "        self.points.index.set_names('id', level=None, inplace=True)\n",
    "        self.points.to_csv(filename+\".csv\", index=True, encoding='utf-8')\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89fd012",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This class requires a merge command first, to group events which overlap with multiple other points. Once grouped together, the selection command allows the duplicates to be filtered out either by uncertainty, as described above, which is the recommended selection, or by intensity, which will choose the closest intensity values. <br>\n",
    "Lastly, the points command is used to determine the three-dimensional localizations, by combining the most accurate information from the position matched events.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c48960",
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtered = filtering(Values)\n",
    "Filtered = Filtered.merge()\n",
    "Filtered = Filtered.selection(selection_type=\"uncertainty\")\n",
    "Filtered = Filtered.points()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f63b22",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The following code demonstrates how many points are contained within the finalized dataframe, as well as the minimum value for the positional uncertainty in the X and Y directions, as well as the minimum positional uncertainty in the Z direction.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Filtered.points[\"X [nm]\"]))\n",
    "print(min(Filtered.points[\"Uncertainty Z [nm]\"]))\n",
    "print(min(Filtered.points[\"Uncertainty XY [nm]\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203b944",
   "metadata": {},
   "source": [
    "# Plotting;\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This class is used as a convenient plotting tool which expedites plotting the results from any of the described classes in both two and three-dimensional options. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class plotting(object):\n",
    "    \"\"\"\n",
    "    This file is part of the Multi-Orientation MAXWELL software\n",
    "    \n",
    "    File author(s): Sierra Dean <ccnd@live.com>\n",
    "    \n",
    "    Distributed under the GPLv3 Licence.\n",
    "    See accompanying file LICENSE.txt or copy at\n",
    "        http://www.gnu.org/licenses/gpl-3.0.html\n",
    "        \n",
    "    source: https://github.com/SierraD/Multi-Orientation-Maxwell\n",
    "    \n",
    "    Last Updated: Sept 26 2024\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        A technique to visualize the 2D and 3D results obtained from the method.\n",
    "        \n",
    "        Attributes:\n",
    "        data: \n",
    "            The data previously developed and contained within the various classes.\n",
    " \n",
    "        Return:\n",
    "            A plot displaying the values as specified.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        return\n",
    "    \n",
    "    def onecolumn(self, axis=\"XY\", dimensions=2, scale_by_size=False, show_error=False):\n",
    "        \"\"\"\n",
    "        A technique to visualize the 3D results as a single plot of two dimensional data \n",
    "        with the orientation specified, inlcuding the error bars for the localization.\n",
    "        \n",
    "        Attributes:\n",
    "        dimensions: int 2 or 3 \n",
    "            The dimensions of the data, either 2D for the XY/XZ data or 3D for the finalized data.\n",
    "        scale_by_size: list [100, 100], etc\n",
    "            The dimensions to be used for the width and height of the figure.\n",
    "        show_error: bool\n",
    "            True or False for showing the error bars.\n",
    " \n",
    "        Return:\n",
    "            A plot displaying the localizations as a single plot of two-dimensional data.\n",
    "        \"\"\"\n",
    "        fig = plotly.graph_objects.Figure()\n",
    "        if type(scale_by_size) == list:\n",
    "            fig.update_layout(autosize=False, width=scale_by_size[0], height=scale_by_size[1])\n",
    "        if axis == \"XY\":\n",
    "            if dimensions == 2:\n",
    "                fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxy[\"X_XY\"], y=self.data.dfxy[\"Y_XY\"], name=\"XY\",\n",
    "                                         error_x=dict(type='data', array=self.data.dfxy[\"U_XY\"], \n",
    "                                                      visible=show_error, width=1, color=\"gray\"), \n",
    "                                         error_y=dict(type='data', array=self.data.dfxy[\"U_XY\"], \n",
    "                                                      visible=show_error, width=1, color=\"gray\"),\n",
    "                                         marker=dict(color=\"Red\", size=2), mode=\"markers\"))\n",
    "                fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxz[\"X_XZ\"], y=self.data.dfxz[\"Y_XZ\"], name=\"XZ\",\n",
    "                                     error_x=dict(type='data', array=self.data.dfxz[\"U_X\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                         marker=dict(color=\"Blue\", size=2), mode=\"markers\"))\n",
    "\n",
    "            elif dimensions == 3:\n",
    "                fig.add_trace(plotly.graph_objects.Scatter(x=self.data.points[\"X [nm]\"], y=self.data.points[\"Y [nm]\"],\n",
    "                                     error_x=dict(type='data', array=self.data.points[\"Uncertainty XY [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                     error_y=dict(type='data', array=self.data.points[\"Uncertainty XY [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"),   \n",
    "                                     marker=dict(color=\"Black\", size=2), mode=\"markers\", showlegend=False))\n",
    "        elif axis == \"XZ\":\n",
    "            if dimensions == 2:\n",
    "                fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxy[\"X_XY\"], y=self.data.dfxy[\"Z_XY\"], \n",
    "                                         error_x=dict(type='data', array=self.data.dfxy[\"U_XY\"], \n",
    "                                                      visible=show_error, width=1, color=\"gray\"), \n",
    "                                         marker=dict(color=\"Red\", size=2), mode=\"markers\", showlegend=False))\n",
    "                fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxz[\"X_XZ\"], y=self.data.dfxz[\"Z_XZ\"], \n",
    "                                         error_x=dict(type='data', array=self.data.dfxz[\"U_X\"], \n",
    "                                                      visible=show_error, width=1, color=\"gray\"), \n",
    "                                         error_y=dict(type='data', array=self.data.dfxz[\"U_Z\"], \n",
    "                                                      visible=show_error, width=1, color=\"gray\"),    \n",
    "                                         marker=dict(color=\"Blue\", size=2), mode=\"markers\", showlegend=False))\n",
    "            elif dimensions == 3:\n",
    "                fig.add_trace(plotly.graph_objects.Scatter(x=self.data.points[\"X [nm]\"], y=self.data.points[\"Z [nm]\"],\n",
    "                                     error_x=dict(type='data', array=self.data.points[\"Uncertainty XY [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                     error_y=dict(type='data', array=self.data.points[\"Uncertainty Z [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"),   \n",
    "                                     marker=dict(color=\"Black\", size=2), mode=\"markers\", showlegend=False))\n",
    "        elif axis == \"YZ\":\n",
    "            if dimensions == 2:\n",
    "                fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxy[\"Y_XY\"], y=self.data.dfxy[\"Z_XY\"], \n",
    "                                         error_x=dict(type='data', array=self.data.dfxy[\"U_XY\"], \n",
    "                                                      visible=show_error, width=1, color=\"gray\"), \n",
    "                                         marker=dict(color=\"Red\", size=2), mode=\"markers\", showlegend=False))\n",
    "                fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxz[\"Y_XZ\"], y=self.data.dfxz[\"Z_XZ\"], \n",
    "                                         error_y=dict(type='data', array=self.data.dfxz[\"U_Z\"], \n",
    "                                                      visible=show_error, width=1, color=\"gray\"),    \n",
    "                                         marker=dict(color=\"Blue\", size=2), mode=\"markers\", showlegend=False))\n",
    "            elif dimensions == 3:\n",
    "                fig.add_trace(plotly.graph_objects.Scatter(x=self.data.points[\"Y [nm]\"], y=self.data.points[\"Z [nm]\"],\n",
    "                                     error_x=dict(type='data', array=self.data.points[\"Uncertainty XY [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                     error_y=dict(type='data', array=self.data.points[\"Uncertainty Z [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"),   \n",
    "                                     marker=dict(color=\"Black\", size=2), mode=\"markers\", showlegend=False))\n",
    "        fig.show()\n",
    "        return self\n",
    "    \n",
    "    def tricolumn(self, dimensions=2, scale_by_size=False, show_error=False):\n",
    "        \"\"\"\n",
    "        A technique to visualize the 3D results as three columns of two dimensional data, \n",
    "        inlcuding the error bars for the localization.\n",
    "        \n",
    "        Attributes:\n",
    "        dimensions: int 2 or 3 \n",
    "            The dimensions of the data, either 2D for the XY/XZ data or 3D for the finalized data.\n",
    " \n",
    "        Return:\n",
    "            A plot displaying the localizations as three columns of two-dimensioanl data.\n",
    "        \"\"\"\n",
    "        fig = plotly.subplots.make_subplots(rows=1, cols=3, subplot_titles=(\"XY Orientation\", \n",
    "                                                                            \"XZ Orientation\", \n",
    "                                                                            \"YZ Orientation\"), \n",
    "                                            horizontal_spacing = 0.1)\n",
    "        if type(scale_by_size) == list:\n",
    "            fig.update_layout(autosize=False, width=scale_by_size[0], height=scale_by_size[1])\n",
    "        if dimensions == 2:\n",
    "            fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxy[\"X_XY\"], y=self.data.dfxy[\"Y_XY\"], name=\"XY\",\n",
    "                                     error_x=dict(type='data', array=self.data.dfxy[\"U_XY\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                     error_y=dict(type='data', array=self.data.dfxy[\"U_XY\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"),   \n",
    "                                     marker=dict(color=\"Red\", size=2), mode=\"markers\"), row=1, col=1)\n",
    "            fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxz[\"X_XZ\"], y=self.data.dfxz[\"Y_XZ\"], name=\"XZ\",\n",
    "                                     error_x=dict(type='data', array=self.data.dfxz[\"U_X\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                 marker=dict(color=\"Blue\", size=2), mode=\"markers\"), row=1, col=1)\n",
    "            fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxy[\"X_XY\"], y=self.data.dfxy[\"Z_XY\"], \n",
    "                                     error_x=dict(type='data', array=self.data.dfxy[\"U_XY\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                     marker=dict(color=\"Red\", size=2), mode=\"markers\", showlegend=False), row=1, col=2)\n",
    "            fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxz[\"X_XZ\"], y=self.data.dfxz[\"Z_XZ\"], \n",
    "                                     error_x=dict(type='data', array=self.data.dfxz[\"U_X\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                     error_y=dict(type='data', array=self.data.dfxz[\"U_Z\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"),    \n",
    "                                     marker=dict(color=\"Blue\", size=2), mode=\"markers\", showlegend=False), row=1, col=2)\n",
    "            fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxy[\"Y_XY\"], y=self.data.dfxy[\"Z_XY\"], \n",
    "                                     error_x=dict(type='data', array=self.data.dfxy[\"U_XY\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                     marker=dict(color=\"Red\", size=2), mode=\"markers\", showlegend=False), row=1, col=3)\n",
    "            fig.add_trace(plotly.graph_objects.Scatter(x=self.data.dfxz[\"Y_XZ\"], y=self.data.dfxz[\"Z_XZ\"], \n",
    "                                     error_y=dict(type='data', array=self.data.dfxz[\"U_Z\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"),    \n",
    "                                     marker=dict(color=\"Blue\", size=2), mode=\"markers\", showlegend=False), row=1, col=3)\n",
    "        elif dimensions == 3:\n",
    "            fig.add_trace(plotly.graph_objects.Scatter(x=self.data.points[\"X [nm]\"], y=self.data.points[\"Y [nm]\"],\n",
    "                                     error_x=dict(type='data', array=self.data.points[\"Uncertainty XY [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                     error_y=dict(type='data', array=self.data.points[\"Uncertainty XY [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"),   \n",
    "                                     marker=dict(color=\"Black\", size=2), mode=\"markers\", showlegend=False), row=1, col=1)\n",
    "            fig.add_trace(plotly.graph_objects.Scatter(x=self.data.points[\"X [nm]\"], y=self.data.points[\"Z [nm]\"],\n",
    "                                     error_x=dict(type='data', array=self.data.points[\"Uncertainty XY [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                     error_y=dict(type='data', array=self.data.points[\"Uncertainty Z [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"),   \n",
    "                                     marker=dict(color=\"Black\", size=2), mode=\"markers\", showlegend=False), row=1, col=2)\n",
    "            fig.add_trace(plotly.graph_objects.Scatter(x=self.data.points[\"Y [nm]\"], y=self.data.points[\"Z [nm]\"],\n",
    "                                     error_x=dict(type='data', array=self.data.points[\"Uncertainty XY [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"), \n",
    "                                     error_y=dict(type='data', array=self.data.points[\"Uncertainty Z [nm]\"], \n",
    "                                                  visible=show_error, width=1, color=\"gray\"),   \n",
    "                                     marker=dict(color=\"Black\", size=2), mode=\"markers\", showlegend=False), row=1, col=3)\n",
    "        fig.update_xaxes(title_text=\"X [nm]\", row=1, col=1, title_standoff = 0)\n",
    "        fig.update_yaxes(title_text=\"Y [nm]\", row=1, col=1, title_standoff = 0)\n",
    "        fig.update_xaxes(title_text=\"X [nm]\", row=1, col=2, title_standoff = 0)\n",
    "        fig.update_yaxes(title_text=\"Z [nm]\", row=1, col=2, title_standoff = 0)\n",
    "        fig.update_xaxes(title_text=\"Y [nm]\", row=1, col=3, title_standoff = 0)\n",
    "        fig.update_yaxes(title_text=\"Z [nm]\", row=1, col=3, title_standoff = 0)\n",
    "        fig.show()\n",
    "        return self\n",
    "    \n",
    "    def tricolumn_sigma(self, dimensions=2, scale_by_size=False):\n",
    "        \"\"\"\n",
    "        A technique to visualize the 3D results as three columns of two dimensional data, \n",
    "        with the size of the markers represented as the sigma value obtained from ThunderSTORM.\n",
    "        \n",
    "        Attributes:\n",
    "        dimensions: int 2 or 3 \n",
    "            The dimensions of the data, either 2D for the XY/XZ data or 3D for the finalized data.\n",
    " \n",
    "        Return:\n",
    "            A plot displaying the localizations as three columns of 2D data visualized as the ThunderSTORM sigma value.\n",
    "        \"\"\"\n",
    "        if dimensions == 2:\n",
    "            fig = plotly.subplots.make_subplots(rows=1, cols=2, subplot_titles=(\"XY Orientation\", \"XZ Orientation\"))\n",
    "            if type(scale_by_size) == list:\n",
    "                fig.update_layout(autosize=False, width=scale_by_size[0], height=scale_by_size[1])\n",
    "            left_x_xy = self.data.dfxy[\"X_XY\"] - self.data.dfxy[\"S_XY\"]\n",
    "            right_x_xy = self.data.dfxy[\"X_XY\"] + self.data.dfxy[\"S_XY\"]\n",
    "            left_y = self.data.dfxy[\"Y_XY\"] - self.data.dfxy[\"S_XY\"]\n",
    "            right_y = self.data.dfxy[\"Y_XY\"] + self.data.dfxy[\"S_XY\"]\n",
    "            left_x_xz = self.data.dfxz[\"X_XZ\"] - self.data.dfxz[\"S_X\"]\n",
    "            right_x_xz = self.data.dfxz[\"X_XZ\"] + self.data.dfxz[\"S_X\"]\n",
    "            left_z = self.data.dfxz[\"Z_XZ\"] - self.data.dfxz[\"S_Z\"]\n",
    "            right_z = self.data.dfxz[\"Z_XZ\"] + self.data.dfxz[\"S_Z\"]\n",
    "            for i in range(0, len(left_x_xy)): \n",
    "                fig.add_shape(type=\"circle\", x0=left_x_xy[i], x1=right_x_xy[i], y0=left_y[i], y1=right_y[i], \n",
    "                              fillcolor=\"red\", line_color=\"red\", opacity=0.2, row=1, col=1)\n",
    "            for j in range(0, len(left_x_xz)): \n",
    "                fig.add_shape(type=\"circle\", x0=left_x_xz[j], x1=right_x_xz[j], y0=left_z[j], y1=right_z[j], \n",
    "                              fillcolor=\"Blue\", line_color=\"Blue\", opacity=0.2, row=1, col=2)\n",
    "            fig.update_xaxes(range=[min(left_x_xy)+((min(right_x_xy)-max(left_x_xy))/4),\n",
    "                                max(right_x_xy)-((min(right_x_xy)-max(left_x_xy))/4)], row=1, col=1)\n",
    "            fig.update_yaxes(range=[min(left_y)+((min(right_y)-max(left_y))/4),\n",
    "                                max(right_y)-((min(right_y)-max(left_y))/4)], row=1, col=1)\n",
    "            fig.update_xaxes(range=[min(left_x_xz)+((min(right_x_xz)-max(left_x_xz))/4),\n",
    "                                max(right_x_xz)-((min(right_x_xz)-max(left_x_xz))/4)], row=1, col=2)\n",
    "            fig.update_yaxes(range=[min(left_z)+((min(right_z)-max(left_z))/4),\n",
    "                                max(right_z)-((min(right_z)-max(left_z))/4)], row=1, col=2)\n",
    "            fig.show()\n",
    "        elif dimensions == 3:\n",
    "            fig = plotly.subplots.make_subplots(rows=1, cols=3, subplot_titles=(\"XY Orientation\", \n",
    "                                                                                \"XZ Orientation\", \n",
    "                                                                                \"YZ Orientation\"))\n",
    "            if type(scale_by_size) == list:\n",
    "                fig.update_layout(autosize=False, width=scale_by_size[0], height=scale_by_size[1])\n",
    "            left_x = self.data.points[\"X [nm]\"] - self.data.points[\"Sigma XY [nm]\"]\n",
    "            right_x = self.data.points[\"X [nm]\"] + self.data.points[\"Sigma XY [nm]\"]\n",
    "            left_y = self.data.points[\"Y [nm]\"] - self.data.points[\"Sigma XY [nm]\"]\n",
    "            right_y = self.data.points[\"Y [nm]\"] + self.data.points[\"Sigma XY [nm]\"]\n",
    "            left_z = self.data.points[\"Z [nm]\"] - self.data.points[\"Sigma Z [nm]\"]\n",
    "            right_z = self.data.points[\"Z [nm]\"] + self.data.points[\"Sigma Z [nm]\"]\n",
    "            for i in range(0, len(left_x)): \n",
    "                fig.add_shape(type=\"circle\", x0=left_x[i], x1=right_x[i], y0=left_y[i], y1=right_y[i], \n",
    "                              fillcolor=\"Black\", line_color=\"Black\", opacity=0.4, row=1, col=1)\n",
    "                fig.add_shape(type=\"circle\", x0=left_x[i], x1=right_x[i], y0=left_z[i], y1=right_z[i], \n",
    "                              fillcolor=\"Black\", line_color=\"Black\", opacity=0.4, row=1, col=2)\n",
    "                fig.add_shape(type=\"circle\", x0=left_y[i], x1=right_y[i], y0=left_z[i], y1=right_z[i], \n",
    "                              fillcolor=\"Black\", line_color=\"Black\", opacity=0.4, row=1, col=3)\n",
    "            fig.update_xaxes(range=[min(left_x)+((min(right_x)-max(left_x))/4),\n",
    "                                max(right_x)-((min(right_x)-max(left_x))/4)], row=1, col=1)\n",
    "            fig.update_yaxes(range=[min(left_y)+((min(right_y)-max(left_y))/4),\n",
    "                                max(right_y)-((min(right_y)-max(left_y))/4)], row=1, col=1)\n",
    "            fig.update_xaxes(range=[min(left_x)+((min(right_x)-max(left_x))/4),\n",
    "                                max(right_x)-((min(right_x)-max(left_x))/4)], row=1, col=2)\n",
    "            fig.update_yaxes(range=[min(left_z)+((min(right_z)-max(left_z))/4),\n",
    "                                max(right_z)-((min(right_z)-max(left_z))/4)], row=1, col=2)\n",
    "            fig.update_xaxes(range=[min(left_y)+((min(right_y)-max(left_y))/4),\n",
    "                                max(right_y)-((min(right_y)-max(left_y))/4)], row=1, col=3)\n",
    "            fig.update_yaxes(range=[min(left_z)+((min(right_z)-max(left_z))/4),\n",
    "                                max(right_z)-((min(right_z)-max(left_z))/4)], row=1, col=3)\n",
    "            fig.show()\n",
    "        return self\n",
    "    \n",
    "    def three_dimensional(self, dimensions=2, scale_by_size=False):\n",
    "        \"\"\"\n",
    "        A technique to visualize the 3D results of the localizations.\n",
    "        \n",
    "        Attributes:\n",
    "        dimensions: int 2 or 3 \n",
    "            The dimensions of the data, either 2D for the XY/XZ data or 3D for the finalized data.\n",
    " \n",
    "        Return:\n",
    "            A 3D visualization of the localizations.\n",
    "        \"\"\"\n",
    "        fig = plotly.graph_objects.Figure()\n",
    "        if type(scale_by_size) == list:\n",
    "            fig.update_layout(width=scale_by_size[0], height=scale_by_size[1])\n",
    "        if dimensions == 2:\n",
    "            fig.add_trace(plotly.graph_objects.Scatter3d(x=self.data.dfxy[\"X_XY\"], \n",
    "                                                         y=self.data.dfxy[\"Y_XY\"], \n",
    "                                                         z=self.data.dfxy[\"Z_XY\"],\n",
    "                                       name = \"XY\",\n",
    "                                       marker=dict(color=\"#FF2D00\", size=2), mode=\"markers\"))\n",
    "            fig.add_trace(plotly.graph_objects.Scatter3d(x=self.data.dfxz[\"X_XZ\"], \n",
    "                                                         y=self.data.dfxz[\"Y_XZ\"], \n",
    "                                                         z=self.data.dfxz[\"Z_XZ\"],\n",
    "                                       name = \"XZ\",\n",
    "                                       marker=dict(color=\"#001BFF\", size=2), mode=\"markers\"))\n",
    "        elif dimensions == 3:\n",
    "            fig.add_trace(plotly.graph_objects.Scatter3d(x=self.data.points[\"X [nm]\"], \n",
    "                                                         y=self.data.points[\"Y [nm]\"], \n",
    "                                                         z=self.data.points[\"Z [nm]\"],\n",
    "                                       marker=dict(color=\"#000000\", size=2), mode=\"markers\"))\n",
    "        fig.show()\n",
    "        return self\n",
    "    \n",
    "    def three_dimensional_sigma(self, dimensions=3, scale_by_size=False):\n",
    "        \"\"\"\n",
    "        A technique to visualize the 3D results of the localizations, \n",
    "        with the size represented as the sigma value obtained from ThunderSTORM.\n",
    "        \n",
    "        Attributes:\n",
    "        dimensions: int 2\n",
    "            The dimensions of the data, should be 3D for the finalized data.\n",
    " \n",
    "        Return:\n",
    "            A 3D visualization of the localizations with the size represented as the \n",
    "            sigma value obtained from ThunderSTORM.\n",
    "        \"\"\"\n",
    "        if dimensions != 3:\n",
    "            raise ValueError(\"The PSF is only three dimensional after the data has been converted to three dimensions.\")\n",
    "        u, v = numpy.mgrid[0:2*numpy.pi:20j, 0:numpy.pi:10j]\n",
    "        fig = plotly.subplots.make_subplots(rows=1, cols=1, specs=[[{'is_3d': True}]])\n",
    "        if type(scale_by_size) == list:\n",
    "            fig.update_layout(width=scale_by_size[0], height=scale_by_size[1])\n",
    "        for i in range(0,len(self.data.points[\"X [nm]\"])):\n",
    "            x = self.data.points[\"Sigma XY [nm]\"][i]*numpy.cos(u)*numpy.sin(v)+self.data.points[\"X [nm]\"][i]\n",
    "            y = self.data.points[\"Sigma XY [nm]\"][i]*numpy.sin(u)*numpy.sin(v)+self.data.points[\"Y [nm]\"][i]\n",
    "            z = self.data.points[\"Sigma Z [nm]\"][i]*numpy.cos(v)+self.data.points[\"Z [nm]\"][i]\n",
    "            fig.add_trace(plotly.graph_objects.Surface(x=x, y=y, z=z, opacity=0.5), 1, 1)\n",
    "        fig.update_traces(showscale=False)\n",
    "        fig.update_layout(scene = dict(xaxis_title=\"X [nm]\", yaxis_title=\"Y [nm]\", zaxis_title=\"Z [nm]\"))\n",
    "        fig.show()\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23922b8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The code to initiate the Plotting.py class. This class requires one parameter, which is a dataframe of localizations created by any of the described classes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data from Filtering.py:\n",
    "Visualize = plotting(Filtered)\n",
    "# For data from Overlap.py:\n",
    "Overlap_Visualize = plotting(Values) \n",
    "# For data from Preparation.py:\n",
    "Prepare_Visualize = plotting(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1deb1a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The Tricolumn command can be used to visualize the data using three two-dimensional plots, showing XY, XZ, and YZ orientations. For this command, if the data being plotted is the results from the Overlap.py or Preparation.py classes, the dimensions option should be set to 2, as the data is two sets of two dimensional data. Conversely, if the data being plotted is from the Filtering.py class, the dimensions option should be set to 3, as the data is three dimensional. <br>\n",
    "The following are Tricolumn plots obtained from the dataframe obtained from Filtering.py (Top), Overlap.py (Middle), and Preparation.py (Bottom). For the plots containing localizations from various orientations, the XY is depicted in red, and XZ in blue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23144c8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Visualize = Visualize.tricolumn(dimensions = 3)\n",
    "Overlap_Visualize = Overlap_Visualize.tricolumn(dimensions = 2)\n",
    "Prepare_Visualize = Prepare_Visualize.tricolumn(dimensions = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaeb91a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For the Tricolumn command, the data is visualized with an assigned point size, to save computational time, but using the Tricolumn_Sigma command instead, the points will be visualized by their individual standard deviation values of the Gaussian fitted on the peak during localization, understood as the unique point spread function of the localization, which is more computationally demanding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e5b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize = Visualize.tricolumn_sigma(dimensions = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fece4b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Similarly, the Three_dimensional command can be used to visualize all localizations from the Filtering.py dataframe, with an arbitrary point size. The Three_dimensional_sigma command allows the points to be visualized from their point spread functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize = Visualize.three_dimensional(dimensions = 3, scale_by_size=[800, 800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ff6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize = Visualize.three_dimensional_sigma(dimensions = 3, scale_by_size=[800, 800])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9e73d",
   "metadata": {},
   "source": [
    "# Precision;\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This class is used to determine the method precision for the analyzed data. The precision can be generated for the Bkgstd, Uncertainty, or Sigma columns from the Filtering.py class, as these values are all standard deviation values.<br>\n",
    "To perform the analysis, a histogram is created of the chosen data, which is then fit with PDF fittings from common distributions to find the most accurate fit for the data. The PDF provides insights into the histogram properties. In the case of standard deviation results, the precision can be obtained by determining the most likely value of the standard deviation, which will be the peak of the PDF fitting applied to the histogram.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800af33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class precision(object):\n",
    "    \"\"\"\n",
    "    This file is part of the Multi-Orientation MAXWELL software\n",
    "    \n",
    "    File author(s): Sierra Dean <ccnd@live.com>\n",
    "    \n",
    "    Distributed under the GPLv3 Licence.\n",
    "    See accompanying file LICENSE.txt or copy at\n",
    "        http://www.gnu.org/licenses/gpl-3.0.html\n",
    "        \n",
    "    source: https://github.com/SierraD/Multi-Orientation-Maxwell\n",
    "    \n",
    "    Last Updated: Sept 26 2024\n",
    "    \"\"\"\n",
    "    def __init__(self, data, data_type=\"Uncertainty XY [nm]\"):\n",
    "        \"\"\"\n",
    "        A technique to automatically obtain the precision of the method. \n",
    "        \n",
    "        The data returned by ThunderSTORM (i.e. Uncertainty, Sigma) are standard deviation \n",
    "        values obtained by fitting the fluorescent spot many times, and therefore the sigma\n",
    "        value obtained from this deviation can be interpreted as the method precision. \n",
    "        \n",
    "        To calculate the method precision, the histogram of the data is automatically \n",
    "        compared with Probability Distribution Functions (PDF) from known distributions \n",
    "        (i.e. Normal, Chi2, Exponential, etc.), then the best fit is selected by minimizing \n",
    "        the Sum of the Square Error between the histogram and the PDF.\n",
    "        \n",
    "        \n",
    "        Attributes:\n",
    "        data: \n",
    "            The data previously developed and contained within the filtering class.\n",
    " \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.data_type = data_type\n",
    "        self.points = self.data.points\n",
    "        self.height = data.points[data_type].values\n",
    "        self.best_fit = \"norm\"\n",
    "        return\n",
    "    \n",
    "    def get_precision(self, distribution=fitter.get_common_distributions()):\n",
    "        \"\"\"\n",
    "        A technique to generate a histogram from specified data, and compare with PDF fittings from \n",
    "        known distributions. \n",
    "        \n",
    "        A plot of the data is automatically generated, and up to five PDF distributions, selected \n",
    "        by minizming the Sum of the Square Error, are plotted in conjunction.\n",
    "                \n",
    "        Attributes:\n",
    "        data: str \"Uncertainty XY [nm]\", \"Uncertainty Z [nm]\", \"Sigma XY [nm]\", \"Sigma Z [nm]\", etc.\n",
    "            The data to be used for the histogram and the fitting.\n",
    "        distribution: str \"norm\", \"lognorm\", get_common_distributions(), get_distributions(), etc.\n",
    "            The type of distribution to be used for the fitting.\n",
    "            If none specified, all common distributions (10) will be compared.\n",
    "            \n",
    "        Return:\n",
    "            None. Will modify the data established in place.\n",
    "        \"\"\"\n",
    "        fit = fitter.Fitter(self.height, distributions=distribution)\n",
    "        fit.fit()\n",
    "        print(fit.summary())\n",
    "        self.best_fit = list(fit.get_best().keys())[0]\n",
    "        print(\"\\nBest Fit Selected: \", self.best_fit)\n",
    "        return self\n",
    "    \n",
    "    def fitted_precision(self, kde=False, bins=30, distribution=False):\n",
    "        \"\"\"\n",
    "        A technique to automatically obtain the precision of data using either the determined or \n",
    "        specified best fit from known distributions. \n",
    "        \n",
    "        A plot of the data is automatically generated, with the specified PDF plotted in \n",
    "        conjunction, in addition to displaying useful values related to method precision.\n",
    "        \n",
    "        Attributes:\n",
    "        kde: bool \n",
    "            The decision to also display the Kernel Density Estimation (KDE) of the histogram.\n",
    "        bins: int\n",
    "            The number of bins to be used for the final histogram.\n",
    "        distribution: False or str \"norm\", \"lognorm\", etc.\n",
    "            Eiter False, which will automatically use the best fit determined by the \n",
    "            'get_precision' method, or a string specifying the known distribution to \n",
    "            perform a PDF comparison with.\n",
    "\n",
    "        Return:\n",
    "            A plot displaying the histogram and important variables.\n",
    "        \"\"\"\n",
    "        if distribution == False:\n",
    "            distribution = self.best_fit\n",
    "        else: \n",
    "            distribution = distribution\n",
    "        \n",
    "        fit = fitter.Fitter(self.height, distributions=distribution)\n",
    "        fit.fit()\n",
    "        fit.summary()\n",
    "        xmin, xmax = matplotlib.pyplot.xlim()\n",
    "        self.Xaxis = numpy.linspace(xmin, xmax, 1000)\n",
    "        \n",
    "        params = list(fit.fitted_param[distribution]) \n",
    "        best_fitted = eval(\"scipy.stats.\" + distribution)\n",
    "        fitting = best_fitted.pdf(self.Xaxis, *params)\n",
    "        \n",
    "        fig0, ax0 = matplotlib.pyplot.subplots(figsize=(10,5))\n",
    "        ax0.set_title(\"Precision Analysis\")\n",
    "        ax0.set_ylabel(\"Density\")\n",
    "        ax0.set_xlabel(self.data_type)\n",
    "        seaborn.histplot(x=self.points[self.data_type], ax=ax0, stat=\"density\", kde=kde, bins=bins, linewidth=0.5, \n",
    "                     label=(\"N=\"+str(len(self.points[self.data_type]))), \n",
    "                     line_kws={'label': \"Kernel Density Estimation (KDE) Fitting\"})\n",
    "        ax0.plot(self.Xaxis, fitting, color=\"red\", linestyle=\"dashed\", \n",
    "                 label=\"Probability Density Function (PDF) Fitting\")\n",
    "        a = round(self.Xaxis[numpy.where(fitting == fitting.max())][0], 4)\n",
    "        b = round(fitting[numpy.where(fitting == fitting.max())][0], 4)\n",
    "        ax0.scatter(a, b, color=\"red\", label=(\" = \"+str(a)+\" [nm]\"))\n",
    "        ax0.legend()\n",
    "        matplotlib.pyplot.show()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93236539",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The code to initiate the Precision.py class. This class requires a dataframe of three-dimensional localizations, formatted in the Filtering.py class, as well as a column specified by the data_type parameter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "InDepth_Precision = precision(Filtered, data_type=\"Uncertainty Z [nm]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adfe337",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The get_precision command is used to obtain a histogram of the specified data column, then fit the histogram with PDF distributions. If the most accurate distribution is not known, the distribution parameter can be specified as distribution = fitter.get_distributions(), which will fit the histogram with various distributions, which are visible in the Python Fitter Package documentation [https://pypi.org/project/fitter/]. If no distribution is specified, only the Fitter common distributions will be fit. If the best fit option is known, the distribution can be set to save computation time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee36cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "InDepth_Precision = InDepth_Precision.get_precision(distribution=\"norminvgauss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f5e76",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The fitted_precision command is used to fit the histogram with the best fit determined, and to obtain the histogram properties. This command includes the kde parameter, which will also fit the histogram with a Kernel Density Estimate for reference. In addition, the number of histogram bins can be specified with bins, and the best fit distribution determined with the previous command should be specified.<br>\n",
    "Shown below is the histogram and PDF fitting obtained from the Precision.py dataframe, depicting the results from the Uncertainty Z [nm] column. The localizations are depicted as a histogram, with the specified column displayed on the X axis. The number of events is recorded in the legend. The chosen PDF, a Normal Inverse Gaussian for this example, is plotted in red, with the maximum value of the PDF fitting shown as a red circle. The precision is described as the maximum value of the PDF fit on a histogram of standard deviation values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "InDepth_Precision = InDepth_Precision.fitted_precision(kde=False, bins=100, distribution=\"norminvgauss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d8811",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This command can be repeated for other columns, to determine the precision for other localization values as well, such as the positional uncertainty in the X and Y directions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_Precision = precision(Filtered, data_type=\"Uncertainty XY [nm]\")\n",
    "XY_Precision = XY_Precision.get_precision(distribution=\"burr\")\n",
    "XY_Precision = XY_Precision.fitted_precision(kde=False, bins=100, distribution=\"burr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
